# Latent-Semantic-Analysis
This is a project from "Scalable and Distributed Computing" course.


For more detail, read the [report](https://github.com/lephanthutra/Latent-Semantic-Analysis/blob/main/Report%20.pdf).

In this project we use Pyspark to clean the dataset with 69,195,478 rows with applying multithread. After that, we apply TF-IDF and SVD to the data.

The dataset was downloaded and uploaded in .rar files to Google Drive.  We use !gdown function to download the large file in Google Colab.
This can cause some error when you try to download those file directly from Google Drive. You should download the data from the data source and upload the dataset by yourself.


Here is the [Data Source](https://dataverse.harvard.edu/dataset.xhtml;jsessionid=249813e9249f73ffb535aae5dc2b?persistentId=doi%3A10.7910%2FDVN%2FLXQXAO&version=&q=&fileTypeGroupFacet=%22Archive%22&fileTag=&fileSortField=size&fileSortOrder=).


