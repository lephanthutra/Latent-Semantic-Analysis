# Latent-Semantic-Analysis
This is a project from "Scalable and Distributed Computing" course.

In this project we use Pyspark to clean the dataset with 69,195,478 rows with applying multithread. After that, we apply TF-IDF and SVD to the data.

The dataset was downloaded and uploaded in .rar files to Google Drive.  We use !gdown function to download the large file.
This can cause some error when you try download those file directly from Google Drive. You should download and upload dataset by yourself.

Here is the [Data Source](https://dataverse.harvard.edu/dataset.xhtml;jsessionid=249813e9249f73ffb535aae5dc2b?persistentId=doi%3A10.7910%2FDVN%2FLXQXAO&version=&q=&fileTypeGroupFacet=%22Archive%22&fileTag=&fileSortField=size&fileSortOrder=)


